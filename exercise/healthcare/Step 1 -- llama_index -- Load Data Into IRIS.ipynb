{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and embed your data\n",
    "\n",
    "In this notebook, you will use llama index to load and embed the data. \n",
    "\n",
    "Run the first block below, which will import the needed libraries and environment variables, then load the data from a JSON lines file. This is a file format where each line is a complete JSON object, separated by new lines. This format is particularly useful for handling large datasets or streams of data because it allows for reading, writing, and processing one line (or one JSON object) at a time, rather than needing to load an entire file into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## This code is for academic and educational purposes only. ##\n",
    "## Event: Global Summit 2024 Maryland USA                   ##\n",
    "## InterSystems Corporation 2024 (C)                        ##\n",
    "## Date: June 9th 2024                                      ##\n",
    "##############################################################\n",
    "\n",
    "##### We are going to use llama index that allows us to load and store data from file and put it into iris\n",
    "from llama_index import download_loader\n",
    "from llama_index import SimpleDirectoryReader, StorageContext, ServiceContext\n",
    "from llama_index.readers.json import JSONReader\n",
    "from llama_index.indices.vector_store import VectorStoreIndex\n",
    "from llama_iris import IRISVectorStore\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import os\n",
    "\n",
    "##### Let's load our dataset\n",
    "reader = JSONReader(is_jsonl=True)\n",
    "documents = reader.load_data('./data/healthcare/augmented_notes_100.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next block to see the first 5 documents that were loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Let's see the first 5 documents\n",
    "documents[:5]\n",
    "\n",
    "##### We have already reduced these documents (in Step 0) to just the text and first 100 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to connect to InterSystems IRIS so that the data can be vectorized and stored in an InterSystems IRIS database. The following two blocks configure the connection, and initialize the table where your data will be stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Configuring IRIS\n",
    "# Setup our demo connectivity\n",
    "username = 'demo'\n",
    "password = 'demo' \n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '61209' \n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Here, we connect the dataset into the IRISVectorStore helper\n",
    "vector_store = IRISVectorStore.from_params(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    table_name=\"augmented_notes_llamaindex\",\n",
    "    embed_dim=1536,  # openai embedding dimension\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can connect into the InterSystems IRIS instance and save your data in a vectorized format. Run the following block to complete this step. Vectorizing, or embedding, your data creates numerical representations of the data that capture the semantic properties such that similar meanings are represented by numerically close values. \n",
    "\n",
    "In a RAG setup, embeddings help quickly find relevant documents by measuring the similarity between the embedded vectors of a query and those in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Finally, We can connect into the InterSystems IRIS instance and save our data in a vectorized format\n",
    "##### Below, we setup how we are going to index the vectorized data (using an embeddings model)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,                              ##### These are our clinical notes we loaded up\n",
    "    storage_context=storage_context,        ##### This is our connection to the vector store\n",
    "    show_progress=True,                     ##### Let's see the progress as it happens\n",
    ")\n",
    "\n",
    "##### To interact with our embeddings, we take the query engine from our documents\n",
    "query_engine = index.as_query_engine()      ##### The \"as_query_engine\" is a llama_index directive which lets \n",
    "                                            ##### us search and retrieve based on vector similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out vector search \n",
    "\n",
    "Now that the text documents are loaded, embedded, and stored in the vector database, you can try running a vector search. In the code block below, set `query` equal to \"36 year old patient with a history of pain\" and run the block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = query_engine.query(\"\")\n",
    "import textwrap\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
