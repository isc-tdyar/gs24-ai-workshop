{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## This code is for academic and educational purposes only. ##\n",
    "## Event: Global Summit 2024 Maryland USA                   ##\n",
    "## InterSystems Corporation 2024 (C)                        ##\n",
    "## Date: June 9th 2024                                      ##\n",
    "##############################################################\n",
    "\n",
    "##### We are going to use llama index that allows us to load and store data from file and put it into iris\n",
    "from llama_index import download_loader\n",
    "from llama_index import SimpleDirectoryReader, StorageContext, ServiceContext\n",
    "from llama_index.readers.json import JSONReader\n",
    "from llama_index.indices.vector_store import VectorStoreIndex\n",
    "from llama_iris import IRISVectorStore\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import os\n",
    "\n",
    "##### Let's load our dataset\n",
    "reader = JSONReader(is_jsonl=True)\n",
    "documents = reader.load_data('./data/financial/tweets_all.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='34e355d8-87fe-45c9-8541-696b7b9e72d5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\"note\": \"$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fb16baf-29d6-45e7-99f2-fa83411fb957', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\"note\": \"$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='edf6051c-8107-48fb-be23-a928b25cfb58', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\"note\": \"$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d947118-c7e8-4316-a8d5-6a29e404ede7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\"note\": \"$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='630a26d6-d9b5-43ab-b2e1-f4284e77190f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\"note\": \"$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB\"', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Let's see the first 5 documents\n",
    "documents[:5]\n",
    "\n",
    "##### We have already reduced these documents (in Step 0) to just the text and first 100 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Configuring IRIS\n",
    "# Setup our demo connectivity\n",
    "username = 'demo'\n",
    "password = 'demo' \n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '61209' \n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Here, we connect the dataset into the IRISVectorStore helper\n",
    "vector_store = IRISVectorStore.from_params(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    table_name=\"financial_tweets_llamaindex\",\n",
    "    embed_dim=1536,  # openai embedding dimension\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab5d85920784fce84f388a87d8e5d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2483efb1c0624369a81b91e71445b3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### Finally, We can connect into the iris instance and save our data in a vectorized format\n",
    "## TODO: explain how embeddings work and why we're using them\n",
    "##### Below, we setup how we are going to index the vectorized data (using an embeddings model)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,                              ##### These are our clinical notes we loaded up\n",
    "    storage_context=storage_context,        ##### This is our connection to the vector store\n",
    "    show_progress=True,                     ##### Let's see the progress as it happens\n",
    ")\n",
    "\n",
    "##### To interact with our embeddings, we take the query engine from our documents\n",
    "query_engine = index.as_query_engine()      ##### The \"as_query_engine\" is a llama_index directive which lets \n",
    "                                            ##### us search and retrieve based on vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am unable to provide information on Microsoft earnings based on the context provided.\n"
     ]
    }
   ],
   "source": [
    "##### Now, let's use this against our vector store!\n",
    "\n",
    "response = query_engine.query(\"Can you tell me about microsoft earnings\")\n",
    "import textwrap\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
