{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "# TODO make these searches generic per table\n",
    "class VectorSearch:\n",
    "    def __init__(self) -> None:\n",
    "        self.conn = iris.connect('localhost',51729,'USER','demo', 'demo')\n",
    "        \n",
    "    def search_vector_db_with_embedding(self, query_embedding, top_k:int) -> list:\n",
    "        query = f\"\"\"SELECT TOP 4 data.id\n",
    "                    FROM augmented_notes data\n",
    "                    ORDER BY VECTOR_DOT_PRODUCT(TO_VECTOR(data.embedding), TO_VECTOR(?)) DESC\n",
    "                    \"\"\"\n",
    "        iris_cursor = self.conn.cursor()\n",
    "        iris_cursor.execute(query, [str(query_embedding)])\n",
    "        origin_list = iris_cursor.fetchall()\n",
    "        return origin_list\n",
    "    \n",
    "    def search_q_and_a_docs(self, story_ids: list[str]) -> list:\n",
    "        id_tuple = tuple(story_ids)\n",
    "        print(id_tuple)\n",
    "        query = f\"\"\"SELECT TOP 20 \n",
    "                    FROM augmented_note\n",
    "                    WHERE StoryID IN {id_tuple}\n",
    "                    \"\"\"\n",
    "        iris_cursor = self.conn.cursor()\n",
    "        iris_cursor.execute(query)\n",
    "        resultset = list(iris_cursor.fetchall())\n",
    "        q_and_a_list = [{'question':q_and_a[0], 'answer':q_and_a[1]} for q_and_a in resultset]\n",
    "        return q_and_a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\nmitchko\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_iris\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IRISVector\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# from vector_search import VectorSearch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.7.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\datasets\\DenoisingAutoEncoderDataset.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInputExample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py:141\u001b[0m\n\u001b[0;32m    139\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    140\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 141\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    143\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\nmitchko\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "\n",
    "import streamlit as st\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from vector_search import VectorSearch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "st.header('Vector Search', divider='orange')\n",
    "# model = SentenceTransformer(\"avsolatorio/GIST-Embedding-v0\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header('Settings', divider='orange')\n",
    "    choose_embed = st.radio(\"Choose an embedding model:\",(\"all-MiniLM-L6-v2\",\"avsolatorio/GIST-Embedding-v0\",\"None\"),index=1)\n",
    "    choose_LM = st.radio(\"Choose a language model:\",(\"gpt-3.5-turbo\",\"None\"),index=0)\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [\n",
    "        {\"role\": \"assistant\", \"content\": \"Hi, I'm a chatbot that can access your vector stores. What would you like to know?\"}\n",
    "    ]\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    if msg[\"role\"] == \"assistant\":\n",
    "        st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
    "    else:\n",
    "        st.chat_message(msg[\"role\"]).write(msg[\"content\"].replace(\"$\", \"\\$\"))\n",
    "\n",
    "if prompt := st.chat_input():\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    st.chat_message(\"user\").write(prompt.replace(\"$\", \"\\$\")) # Escaping '$', otherwise Streamlit can interpret it as Latex\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        # openai_api_key=,\n",
    "        model_name='gpt-3.5-turbo'\n",
    "    )\n",
    "    # Create chain. We are using Summary Memory for fewer tokens.\n",
    "    conversation_sum = ConversationChain(\n",
    "        llm=llm,\n",
    "        memory=ConversationSummaryMemory(llm=llm),\n",
    "        verbose=True\n",
    "    )\n",
    "    # Instantiate this custom Python class (vector_search.py) which gives us SQL access to the persisted vector embeddings.\n",
    "    peristent_DB = VectorSearch()\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        #;\n",
    "        # Encode the user's prompt and find the top-k similar questions in the vector DB.\n",
    "        embedding = model.encode(prompt)\n",
    "        documents = peristent_DB.search_vector_db_with_embedding(str(embedding.tolist()), top_k=4)\n",
    "        doc_content_list, doc_id_list = map(list, zip(*documents))\n",
    "        doc_list = [Document(page_content=doc_content, metadata={\"source\": \"local\"}) for doc_content in doc_content_list]\n",
    "        #;\n",
    "        # This can potentially return many large documents, so we should use LangChain to chunk the results:\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=0)\n",
    "        docs = text_splitter.split_documents(doc_list)\n",
    "        #;\n",
    "        q_and_a_docs = peristent_DB.search_q_and_a_docs(doc_id_list)\n",
    "\n",
    "        relevant_docs = [str(doc.page_content)[:250] for doc in docs]\n",
    "\n",
    "        relevant_docs[:]\n",
    "\n",
    "        template = f\"\"\"\n",
    "                    Prompt: {prompt}\n",
    "\n",
    "                    Example Responses: {q_and_a_docs}\n",
    "\n",
    "                    Relevant Documents: {str(relevant_docs)}\n",
    "\n",
    "                    You should only make use of the provided Relevant Documents. They are important information belonging to the user, and it is important that any advice you give is grounded in these documents. If the documents are irrelevant to the question, simply state that you do not have the relevant information available in the database.\n",
    "                \"\"\"\n",
    "        resp = conversation_sum(template)\n",
    "\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": resp['response']})\n",
    "        st.write(resp['response'].replace(\"$\", \"\\$\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
